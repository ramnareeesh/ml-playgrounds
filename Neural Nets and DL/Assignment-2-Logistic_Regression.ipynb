{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for classifying red wine based on quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading CSV dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.4             0.700         0.00             1.9      0.076   \n1               7.8             0.880         0.00             2.6      0.098   \n2               7.8             0.760         0.04             2.3      0.092   \n3              11.2             0.280         0.56             1.9      0.075   \n4               7.4             0.700         0.00             1.9      0.076   \n...             ...               ...          ...             ...        ...   \n1594            6.2             0.600         0.08             2.0      0.090   \n1595            5.9             0.550         0.10             2.2      0.062   \n1596            6.3             0.510         0.13             2.3      0.076   \n1597            5.9             0.645         0.12             2.0      0.075   \n1598            6.0             0.310         0.47             3.6      0.067   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                    11.0                  34.0  0.99780  3.51       0.56   \n1                    25.0                  67.0  0.99680  3.20       0.68   \n2                    15.0                  54.0  0.99700  3.26       0.65   \n3                    17.0                  60.0  0.99800  3.16       0.58   \n4                    11.0                  34.0  0.99780  3.51       0.56   \n...                   ...                   ...      ...   ...        ...   \n1594                 32.0                  44.0  0.99490  3.45       0.58   \n1595                 39.0                  51.0  0.99512  3.52       0.76   \n1596                 29.0                  40.0  0.99574  3.42       0.75   \n1597                 32.0                  44.0  0.99547  3.57       0.71   \n1598                 18.0                  42.0  0.99549  3.39       0.66   \n\n      alcohol  quality  \n0         9.4        5  \n1         9.8        5  \n2         9.8        5  \n3         9.8        6  \n4         9.4        5  \n...       ...      ...  \n1594     10.5        5  \n1595     11.2        6  \n1596     11.0        6  \n1597     10.2        5  \n1598     11.0        6  \n\n[1599 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.99680</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.99700</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.99800</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1594</th>\n      <td>6.2</td>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>2.0</td>\n      <td>0.090</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99490</td>\n      <td>3.45</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>5.9</td>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.062</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>0.99512</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>6.3</td>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.076</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>0.99574</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>5.9</td>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.075</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99547</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>6.0</td>\n      <td>0.310</td>\n      <td>0.47</td>\n      <td>3.6</td>\n      <td>0.067</td>\n      <td>18.0</td>\n      <td>42.0</td>\n      <td>0.99549</td>\n      <td>3.39</td>\n      <td>0.66</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>1599 rows Ã— 12 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df = pd.read_csv(\"winequality-red.csv\")\n",
    "wine_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "quality\n5    681\n6    638\n7    199\n4     53\n8     18\n3     10\nName: count, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df[\"quality\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering the Values of Quality\n",
    "<=6 -> 0\n",
    "greater than 6 -> 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def categorise(x):\n",
    "    if x <= 6:\n",
    "        return 0\n",
    "    return 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "wine_df[\"quality\"] = wine_df[\"quality\"].apply(categorise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "quality\n0    1382\n1     217\nName: count, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df[\"quality\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting Dependent and Independent Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X = wine_df.iloc[:, :-1].values\n",
    "y = wine_df.iloc[:, -1].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting of Dataset into Test and Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 8.4  ,  0.745,  0.11 , ...,  3.19 ,  0.82 ,  9.6  ],\n       [ 7.6  ,  0.43 ,  0.29 , ...,  3.4  ,  0.64 ,  9.5  ],\n       [ 8.4  ,  0.56 ,  0.04 , ...,  3.22 ,  0.44 ,  9.6  ],\n       ...,\n       [ 7.9  ,  0.57 ,  0.31 , ...,  3.29 ,  0.69 ,  9.5  ],\n       [13.   ,  0.47 ,  0.49 , ...,  3.3  ,  0.68 , 12.7  ],\n       [ 9.8  ,  0.98 ,  0.32 , ...,  3.25 ,  0.48 ,  9.4  ]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, ..., 0, 0, 0])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MMS = MinMaxScaler()\n",
    "X_train = MMS.fit_transform(X=X_train)\n",
    "X_test = MMS.fit_transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.33035714, 0.42808219, 0.11      , ..., 0.35433071, 0.27607362,\n        0.18461538],\n       [0.25892857, 0.21232877, 0.29      , ..., 0.51968504, 0.16564417,\n        0.16923077],\n       [0.33035714, 0.30136986, 0.04      , ..., 0.37795276, 0.04294479,\n        0.18461538],\n       ...,\n       [0.28571429, 0.30821918, 0.31      , ..., 0.43307087, 0.19631902,\n        0.16923077],\n       [0.74107143, 0.23972603, 0.49      , ..., 0.44094488, 0.19018405,\n        0.66153846],\n       [0.45535714, 0.5890411 , 0.32      , ..., 0.4015748 , 0.06748466,\n        0.15384615]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.56363636, 0.35227273, 0.5443038 , ..., 0.28431373, 0.42574257,\n        0.42857143],\n       [0.31818182, 0.75      , 0.        , ..., 0.47058824, 0.1980198 ,\n        0.21428571],\n       [0.40909091, 0.14772727, 0.41772152, ..., 0.37254902, 0.5049505 ,\n        0.58928571],\n       ...,\n       [0.24545455, 0.23295455, 0.62025316, ..., 0.47058824, 0.44554455,\n        0.46428571],\n       [1.        , 0.59659091, 0.96202532, ..., 0.06862745, 0.34653465,\n        0.5       ],\n       [0.27272727, 0.30681818, 0.36708861, ..., 0.50980392, 0.30693069,\n        0.19642857]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ->  1027\n",
      "1  ->  172\n"
     ]
    }
   ],
   "source": [
    "unique_val, counts = np.unique(y_train, return_counts=True)\n",
    "for i in range(len(unique_val)):\n",
    "    print(unique_val[i], \" -> \", counts[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, learning_rate=0.01, iter=1000):\n",
    "        self.learnRate = learning_rate\n",
    "        self.epochs = iter\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        no_samples = len(X)\n",
    "        no_features = len(X[0])\n",
    "        self.weights = np.zeros(no_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # forward propagation\n",
    "            predicted = self.predict(X)\n",
    "\n",
    "            # backpropagation\n",
    "            dw = (1/no_samples) * np.dot(X.T, (predicted - y))\n",
    "            db = (1/no_samples) * np.sum(predicted - y)\n",
    "            self.weights -= self.learnRate * dw\n",
    "            self.bias -= self.learnRate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        # forward propagation\n",
    "        weighted_sum = np.dot(X, self.weights) + self.bias\n",
    "        predicted = self.sigmoid(weighted_sum)\n",
    "        return predicted\n",
    "\n",
    "    def test(self, X, y):  # passing test sets\n",
    "        y_predicted = self.predict(X)\n",
    "        y_predicted_category = []\n",
    "        for _ in y_predicted:\n",
    "            if _ > 0.5:\n",
    "                y_predicted_category.append(1)\n",
    "            else:\n",
    "                y_predicted_category.append(0)\n",
    "        y_predicted_category = np.array(y_predicted_category)\n",
    "        print(y_predicted_category)\n",
    "        print(y)\n",
    "        print(y_predicted)\n",
    "        correct_pred = 0\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == y_predicted_category[i]:\n",
    "                correct_pred += 1\n",
    "        print(\"Accuracy: \", correct_pred/len(y))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "Regressor = Logistic_Regression()\n",
    "Regressor.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21773676 -0.37894955 -0.08319048 -0.0814842  -0.13685054 -0.22668115\n",
      " -0.18469157 -0.50450284 -0.42244619 -0.08041781 -0.04481006]\n",
      "-0.8976765797581938\n"
     ]
    }
   ],
   "source": [
    "print(Regressor.weights)\n",
    "print(Regressor.bias)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0]\n",
      "[0.12759574 0.13384208 0.16972305 0.14891815 0.1608267  0.14866413\n",
      " 0.15606388 0.12878835 0.14150073 0.13876442 0.16338619 0.14288569\n",
      " 0.14575773 0.11025316 0.15378004 0.1505866  0.18599835 0.16176694\n",
      " 0.16206823 0.1492386  0.17142398 0.14583507 0.16191154 0.15464428\n",
      " 0.14103607 0.13155354 0.13759969 0.15967685 0.13932399 0.15221239\n",
      " 0.16475866 0.13669514 0.13691871 0.13130918 0.12517754 0.14166185\n",
      " 0.13471537 0.12912202 0.16309191 0.16554587 0.13306974 0.15341641\n",
      " 0.16550357 0.13962337 0.16866935 0.12645773 0.18210588 0.14129045\n",
      " 0.13599017 0.15326159 0.17212657 0.17058064 0.13404629 0.11344862\n",
      " 0.13776511 0.14200939 0.13098737 0.15443463 0.15793823 0.14642094\n",
      " 0.12511994 0.15437125 0.16239773 0.1536111  0.13454065 0.17172836\n",
      " 0.15678317 0.11396352 0.15497544 0.13582737 0.11399371 0.1520016\n",
      " 0.13750376 0.14867234 0.14939464 0.16395634 0.15616552 0.14197891\n",
      " 0.12868682 0.15704596 0.12699372 0.13573977 0.15921018 0.17664023\n",
      " 0.15474783 0.14419152 0.14117642 0.14702259 0.1504556  0.13536285\n",
      " 0.12607891 0.12992554 0.15133954 0.16208007 0.17981696 0.11399371\n",
      " 0.17174487 0.1375575  0.16493423 0.15281687 0.15354817 0.15001291\n",
      " 0.1375402  0.16744228 0.14692786 0.14955335 0.14692786 0.13877093\n",
      " 0.16218376 0.14344353 0.15338578 0.15305078 0.17501542 0.12408746\n",
      " 0.14592505 0.13314393 0.16962916 0.16015989 0.13142655 0.16191154\n",
      " 0.1317958  0.14545598 0.13865471 0.13866927 0.13432143 0.16792347\n",
      " 0.13309757 0.12171416 0.16575428 0.16405302 0.14164192 0.15643527\n",
      " 0.1674751  0.17371249 0.14574991 0.13010586 0.12325455 0.14456305\n",
      " 0.16175972 0.17518363 0.16603847 0.13707201 0.18210588 0.12288121\n",
      " 0.15722426 0.14621162 0.17501542 0.158285   0.16615724 0.15965542\n",
      " 0.15037797 0.14813866 0.13877093 0.14660145 0.12900291 0.13500645\n",
      " 0.12098114 0.14192545 0.16506534 0.12358643 0.16248362 0.15803733\n",
      " 0.12880013 0.11906738 0.13414561 0.1430119  0.1362007  0.17159785\n",
      " 0.12518823 0.15285429 0.12430831 0.15449314 0.1435386  0.16804802\n",
      " 0.13558161 0.1879     0.15588186 0.15495881 0.15260967 0.13635231\n",
      " 0.14168137 0.16540903 0.14800244 0.13221018 0.20558788 0.14163828\n",
      " 0.15669434 0.1375402  0.12491867 0.15046518 0.1403373  0.15796268\n",
      " 0.15292494 0.16522774 0.11615745 0.13572871 0.16338619 0.12651849\n",
      " 0.12879423 0.10454016 0.13672003 0.16171043 0.14212633 0.14851897\n",
      " 0.133866   0.12845088 0.15339824 0.15293901 0.1353249  0.14043937\n",
      " 0.15362703 0.16741989 0.13607409 0.09994783 0.14205975 0.14052613\n",
      " 0.1536335  0.16134628 0.15303655 0.13475246 0.13425897 0.15289842\n",
      " 0.1587463  0.14986573 0.14551849 0.12953094 0.1690997  0.14884053\n",
      " 0.1360712  0.1698618  0.12685546 0.14240225 0.13433899 0.12663331\n",
      " 0.17368857 0.14395604 0.14667982 0.15773206 0.19088365 0.14703333\n",
      " 0.13881489 0.13531904 0.1314303  0.14047388 0.16818456 0.13275986\n",
      " 0.16293679 0.1207416  0.12872284 0.15104518 0.14226666 0.14045946\n",
      " 0.14033893 0.13196729 0.16380773 0.15713887 0.13296636 0.15894262\n",
      " 0.13067371 0.11590023 0.1370074  0.16177343 0.13155624 0.15900377\n",
      " 0.15415547 0.14405032 0.13533531 0.11373151 0.15990088 0.1433636\n",
      " 0.1564827  0.16405591 0.17068724 0.14388749 0.13185675 0.14431631\n",
      " 0.19204194 0.14018392 0.18812991 0.14596866 0.14020447 0.1529338\n",
      " 0.16405591 0.11929405 0.1686567  0.12737914 0.18471708 0.16711893\n",
      " 0.15174512 0.13141315 0.14170321 0.14715464 0.15397355 0.14341871\n",
      " 0.13859296 0.14742107 0.15344925 0.12883763 0.13584137 0.14020222\n",
      " 0.14644178 0.13592915 0.14390957 0.14854083 0.16819152 0.12066332\n",
      " 0.14062319 0.1391086  0.13929133 0.12702141 0.12928055 0.14499489\n",
      " 0.1391022  0.1375575  0.1292015  0.17569605 0.17379126 0.18541209\n",
      " 0.14290418 0.17903644 0.12702141 0.15842599 0.15918889 0.14056299\n",
      " 0.14551886 0.11698968 0.16459971 0.16127626 0.12744793 0.12521598\n",
      " 0.17790648 0.13485566 0.1506265  0.15046518 0.14350095 0.13478757\n",
      " 0.13462444 0.14191995 0.1354776  0.12088366 0.13002591 0.15813715\n",
      " 0.15678317 0.13963531 0.14742107 0.14582845 0.13831706 0.13252899\n",
      " 0.15889888 0.15875031 0.16086048 0.16954726 0.16159719 0.17774718\n",
      " 0.16414296 0.13520829 0.1403373  0.17415861 0.14022232 0.15255249\n",
      " 0.14074846 0.15104518 0.16302829 0.13137959 0.14143802 0.14773129\n",
      " 0.12912202 0.16620333 0.13802886 0.1805486  0.13409057 0.14179661\n",
      " 0.15122416 0.14912116 0.14857557 0.12757319 0.14813866 0.16514176\n",
      " 0.14395604 0.15009537 0.16322198 0.14859078 0.11703219 0.14540843\n",
      " 0.16443626 0.16296992 0.11924853 0.16159535 0.17832342 0.14240082\n",
      " 0.15788992 0.15163962 0.13587823 0.11521859 0.12795877 0.1449736\n",
      " 0.1390267  0.12928055 0.10806802 0.14265938]\n",
      "Accuracy:  0.8875\n"
     ]
    }
   ],
   "source": [
    "Regressor.test(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.15871687 0.15707361 0.16815513 ... 0.16046203 0.12500795 0.12465251]\n",
      "Accuracy:  0.8565471226021685\n"
     ]
    }
   ],
   "source": [
    "Regressor.test(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
